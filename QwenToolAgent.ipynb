{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0bd4917-2952-4f78-baba-82d643fe5e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8318c98bc8134851a8480b2c111bdf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fddca6c77d480fb785213997a6211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31437eaf109b433989a56685ba156c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a5d09bd084fdebc593232b83dc396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d25e521bb947e4b669d141856b63d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f93ecd646469886ecc0be245c5504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68cf176230b43b0bec28d43a57ec7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5957b67c172143e6b787b3e828e4dd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e110a4b4e24ef680e8fe43ab89ba27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c0f245c6f54de0a3c40129ba610be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4288e02914b0450c871b408afe7aa64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Qwen2.5-14B-Instruct-1M-4bit\") #mlx-community/Qwen2.5-7B-Instruct-1M-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e12659-c3bd-4c66-a7b8-0b20519ed4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      " <response>Could you please specify the room name and the desired temperature in degrees Celsius?</response>\n",
      " <completed>false</completed>\n",
      "</answer>\n",
      "==========\n",
      "Prompt: 523 tokens, 100.337 tokens-per-sec\n",
      "Generation: 31 tokens, 10.865 tokens-per-sec\n",
      "Peak memory: 12.597 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n <response>Could you please specify the room name and the desired temperature in degrees Celsius?</response>\\n <completed>false</completed>\\n</answer>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temperature_tool(location: str):\n",
    "    \"\"\"\n",
    "    The tool provides the current temperature for a given location (city name). The location needs to be an actual city name and not a [placeholder value]\n",
    "    \n",
    "    Args:\n",
    "        location: The name of the city whose temperature to obtain\n",
    "    Returns:\n",
    "        The temperature of the requested city\n",
    "    \"\"\"\n",
    "    return random.uniform(-5, 20)\n",
    "\n",
    "def heating_tool(room: str, temperature: float):\n",
    "    \"\"\"\n",
    "    The tool sets the temperature for a given roomname. The response is an indication whether the change of temperature was successful or not\n",
    "    \n",
    "    Args:\n",
    "        room: The name of the room where to set the temperature\n",
    "        temperature: The desired temperature in degree celsius\n",
    "    Returns:\n",
    "        a boolean indicating whether setting the temperature was succesfull or not\n",
    "    \"\"\"\n",
    "    return { 'temperature': temperature, 'room': room, 'temperature_changed_successfully': True }\n",
    "\n",
    "def current_time():\n",
    "    \"\"\"Get the current local time as a string.\"\"\"\n",
    "    return str(datetime.now())\n",
    "\n",
    "tools = [temperature_tool, heating_tool, current_time]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "    You are an AI assistant specialized in tool-based problem solving. \n",
    "    Your expertise lies in analyzing questions to determine whether to use specialized tools or provide direct knowledge-based responses, \n",
    "    ensuring optimal resource utilization. If you are lacking information from the user, ask for further information before making a decision. \n",
    "    Never make up any data for your responses but use the provided information only unless it is common knowledge.\n",
    "    Never fill in missing data you need from the customer unless explicitly being asked for it.\n",
    "    Format your response as XML with the following format:\n",
    "    <answer>\n",
    "     <response>[your answer or question or tool invocation]</response>\n",
    "     <tool_call>[if applicable: what tool to call and with which parameters]</tool_call>\n",
    "     <completed>[true is the conversation is done, or false if further customer responses are needed]</completed>\n",
    "    </answer>\n",
    "    \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"change the temperature\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"<answer>\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=False, tools=tools, tokenize=False, continue_final_message=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "335ee67c-d855-4875-ab7d-5c56282f434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      " <response>What is the desired temperature for the bedroom in degrees Celsius?</response>\n",
      " <completed>false</completed>\n",
      "</answer>\n",
      "==========\n",
      "Prompt: 568 tokens, 103.614 tokens-per-sec\n",
      "Generation: 28 tokens, 10.759 tokens-per-sec\n",
      "Peak memory: 12.597 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n <response>What is the desired temperature for the bedroom in degrees Celsius?</response>\\n <completed>false</completed>\\n</answer>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[-1]['content'] += response\n",
    "messages.append({\"role\": \"user\", \"content\": 'in the bedroom'})\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"<answer>\"})\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=False, tools=tools, continue_final_message=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "259e4a60-cd9c-412e-970a-47bac367ad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      " <response>Setting the temperature for the bedroom to 18 degrees Celsius.</response>\n",
      " <completed>false</completed>\n",
      "<tool_call>\n",
      "{\"name\": \"heating_tool\", \"arguments\": {\"room\": \"bedroom\", \"temperature\": 18}}\n",
      "</tool_call>\n",
      "</answer>\n",
      "==========\n",
      "Prompt: 612 tokens, 100.432 tokens-per-sec\n",
      "Generation: 58 tokens, 10.811 tokens-per-sec\n",
      "Peak memory: 12.597 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n <response>Setting the temperature for the bedroom to 18 degrees Celsius.</response>\\n <completed>false</completed>\\n<tool_call>\\n{\"name\": \"heating_tool\", \"arguments\": {\"room\": \"bedroom\", \"temperature\": 18}}\\n</tool_call>\\n</answer>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[-1]['content'] += response\n",
    "messages.append({\"role\": \"user\", \"content\": 'to 18 degree'})\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"<answer>\"})\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=False, tools=tools, continue_final_message=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d6147a2-ea24-4f5a-8dae-878b27e8bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      " <response>The temperature for the bedroom has been successfully set to 18 degrees Celsius.</response>\n",
      " <completed>true</completed>\n",
      "</answer>\n",
      "==========\n",
      "Prompt: 691 tokens, 101.970 tokens-per-sec\n",
      "Generation: 31 tokens, 10.934 tokens-per-sec\n",
      "Peak memory: 12.597 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n <response>The temperature for the bedroom has been successfully set to 18 degrees Celsius.</response>\\n <completed>true</completed>\\n</answer>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[-1]['content'] += response\n",
    "messages.append({\"role\": \"tool\", \"name\": \"heating_tool\", \"content\": \"true\"})\n",
    "messages.append({\"role\": \"assistant\", \"content\": \"<answer>\"})\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=False, tools=tools, continue_final_message=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
