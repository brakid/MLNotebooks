{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning Examples\n",
    "\n",
    "## Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes:\n",
      "\n",
      "1000 :  -210.67100000000065\n",
      "2000 :  -2.9680000000000155\n",
      "3000 :  6.958999999999959\n",
      "4000 :  7.587999999999961\n",
      "5000 :  7.247999999999963\n",
      "6000 :  7.4709999999999654\n",
      "7000 :  7.538999999999965\n",
      "8000 :  7.385999999999964\n",
      "9000 :  7.6209999999999765\n",
      "10000 :  7.241999999999967\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "STATE_SPACE = env.observation_space.n\n",
    "ACTION_SPACE = env.action_space.n\n",
    "\n",
    "NUM_EPISODES = 10000\n",
    "MAX_EPISODE_LENGTH = 100\n",
    "\n",
    "a = 0.1 # alpha = learning rate\n",
    "y = 0.99 # gamma = decay rate\n",
    "\n",
    "MAX_EXPLORATION_RATE = 1\n",
    "MIN_EXPLORATION_RATE = 0.01\n",
    "EXPLORATION_RATE_DECAY = 0.001\n",
    "\n",
    "def decay_exploration_rate(episode):\n",
    "    # exponential decay\n",
    "    #return MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLORATION_RATE_DECAY * episode)\n",
    "    \n",
    "    # linear decay\n",
    "    new_exploration_rate = MAX_EXPLORATION_RATE - (EXPLORATION_RATE_DECAY) * episode\n",
    "    return max(new_exploration_rate, MIN_EXPLORATION_RATE)\n",
    "\n",
    "def should_explore(exploration_rate):\n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    return exploration_rate > exploration_rate_threshold # exploration rate decreases, less frequent larger than random number\n",
    "\n",
    "def get_next_action(q_table, state, exploration_rate):\n",
    "    if should_explore(exploration_rate):\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(q_table[state, :])\n",
    "\n",
    "# discrete state & action space\n",
    "q_table = np.zeros((STATE_SPACE, ACTION_SPACE))\n",
    "exploration_rate = MAX_EXPLORATION_RATE\n",
    "total_rewards = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_EPISODE_LENGTH): \n",
    "        action = get_next_action(q_table, state, exploration_rate)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            q_table[state, action] = (1 - a) * q_table[state, action] + a * reward\n",
    "        else:\n",
    "            q_table[state, action] = (1 - a) * q_table[state, action] + a * (reward + y * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = decay_exploration_rate(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(total_rewards), NUM_EPISODES/1000)\n",
    "\n",
    "print('Average reward per thousand episodes:\\n')\n",
    "for index, r in enumerate(rewards_per_thousand_episodes):\n",
    "    print((index + 1) * 1000, \": \", str(sum(r/1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.1319182 ,   4.50780883,  -6.16080086,  -6.16168415,\n",
       "       -10.87592696, -11.51100366])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[454,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "### Training on last experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 5,192\n",
      "Trainable params: 5,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0\n",
      "Episode 10\n",
      "Episode 20\n",
      "Episode 30\n",
      "Episode 40\n",
      "Episode 50\n",
      "Episode 60\n",
      "Episode 70\n",
      "Episode 80\n",
      "Episode 90\n",
      "Episode 100\n",
      "Episode 110\n",
      "Episode 120\n",
      "Episode 130\n",
      "Episode 140\n",
      "Episode 150\n",
      "Episode 160\n",
      "Episode 170\n",
      "Episode 180\n",
      "Episode 190\n",
      "Average reward per 10 episodes:\n",
      "\n",
      "10 :  -185.0\n",
      "20 :  -190.4\n",
      "30 :  -158.0\n",
      "40 :  -147.2\n",
      "50 :  -126.5\n",
      "60 :  -124.7\n",
      "70 :  -104.0\n",
      "80 :  -79.7\n",
      "90 :  -68.0\n",
      "100 :  -62.6\n",
      "110 :  -53.6\n",
      "120 :  -50.9\n",
      "130 :  -50.9\n",
      "140 :  -51.8\n",
      "150 :  -50.9\n",
      "160 :  -50.9\n",
      "170 :  -51.8\n",
      "180 :  -51.8\n",
      "190 :  -51.8\n",
      "200 :  -53.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "def build_model(state_space, action_space):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(state_space, 10, input_length=1))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='mae', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "np.random.seed(1234)\n",
    "env.seed(1234)\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "STATE_SPACE = env.observation_space.n\n",
    "ACTION_SPACE = env.action_space.n\n",
    "\n",
    "NUM_EPISODES = 200\n",
    "MAX_EPISODE_LENGTH = 50\n",
    "\n",
    "y = 0.99 # gamma = decay rate\n",
    "\n",
    "MAX_EXPLORATION_RATE = 1\n",
    "MIN_EXPLORATION_RATE = 0.01\n",
    "EXPLORATION_RATE_DECAY = 0.01\n",
    "\n",
    "def decay_exploration_rate(episode):\n",
    "    # exponential decay\n",
    "    #return MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLORATION_RATE_DECAY * episode)\n",
    "    \n",
    "    # linear decay\n",
    "    new_exploration_rate = MAX_EXPLORATION_RATE - (EXPLORATION_RATE_DECAY) * episode\n",
    "    return max(new_exploration_rate, MIN_EXPLORATION_RATE)\n",
    "\n",
    "def should_explore(exploration_rate):\n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    return exploration_rate > exploration_rate_threshold # exploration rate decreases, less frequent larger than random number\n",
    "\n",
    "def one_hot_encode(state, state_space):\n",
    "    return np.array([np.eye(state_space)[state]])\n",
    "\n",
    "def encode(state):\n",
    "    return np.array([np.array([state]).reshape((1, 1))])\n",
    "\n",
    "def get_next_action(q_model, state, exploration_rate):\n",
    "    if should_explore(exploration_rate):\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(q_model.predict(encode(state)))\n",
    "\n",
    "# discrete state & action space\n",
    "q_model = build_model(STATE_SPACE, ACTION_SPACE)\n",
    "exploration_rate = MAX_EXPLORATION_RATE\n",
    "total_rewards = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    if episode % 10 == 0:\n",
    "        print('Episode', episode)\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_EPISODE_LENGTH): \n",
    "        action = get_next_action(q_model, state, exploration_rate)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + y * np.max(q_model.predict(encode(new_state)))\n",
    "        current_targets = q_model.predict(encode(state))\n",
    "        current_targets[0][action] = target\n",
    "        q_model.fit(encode(state), current_targets, epochs=1, verbose=0)\n",
    "\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = decay_exploration_rate(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "rewards_per_episodes = np.split(np.array(total_rewards), NUM_EPISODES/10)\n",
    "\n",
    "print('Average reward per 10 episodes:\\n')\n",
    "for index, r in enumerate(rewards_per_episodes):\n",
    "    print((index + 1) * 10, \": \", str(np.average(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100.48277 ,  -99.97023 , -100.81094 , -100.345345, -108.639114,\n",
       "        -109.62755 ]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.predict([328])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "Use a fixed size replay memory. Store ```(state, action, new_state, reward)``` for each action taken and thenùse them as memory blocks during tbhe replay. Thus the model trains on more data than the previous attempt with a single example per training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 5,192\n",
      "Trainable params: 5,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0\n",
      "Episode 10\n",
      "Episode 20\n",
      "Episode 30\n",
      "Episode 40\n",
      "Episode 50\n",
      "Episode 60\n",
      "Episode 70\n",
      "Episode 80\n",
      "Episode 90\n",
      "Episode 100\n",
      "Episode 110\n",
      "Episode 120\n",
      "Episode 130\n",
      "Episode 140\n",
      "Episode 150\n",
      "Episode 160\n",
      "Episode 170\n",
      "Episode 180\n",
      "Episode 190\n",
      "Average reward per 10 episodes:\n",
      "\n",
      "10 :  -78.5\n",
      "20 :  -58.7\n",
      "30 :  -77.6\n",
      "40 :  -89.3\n",
      "50 :  -59.6\n",
      "60 :  -62.3\n",
      "70 :  -46.1\n",
      "80 :  -79.4\n",
      "90 :  -93.8\n",
      "100 :  -73.1\n",
      "110 :  -75.8\n",
      "120 :  -134.3\n",
      "130 :  -101.9\n",
      "140 :  -128.0\n",
      "150 :  -39.8\n",
      "160 :  -74.9\n",
      "170 :  -72.2\n",
      "180 :  -114.5\n",
      "190 :  -146.0\n",
      "200 :  -82.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "def build_model(state_space, action_space):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(state_space, 10, input_length=1))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='mae', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "np.random.seed(1234)\n",
    "env.seed(1234)\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "STATE_SPACE = env.observation_space.n\n",
    "ACTION_SPACE = env.action_space.n\n",
    "\n",
    "NUM_EPISODES = 200\n",
    "MAX_EPISODE_LENGTH = 20\n",
    "\n",
    "y = 0.99 # gamma = decay rate\n",
    "\n",
    "MAX_EXPLORATION_RATE = 1\n",
    "MIN_EXPLORATION_RATE = 0.01\n",
    "EXPLORATION_RATE_DECAY = 0.01\n",
    "\n",
    "def decay_exploration_rate(episode):\n",
    "    # exponential decay\n",
    "    #return MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLORATION_RATE_DECAY * episode)\n",
    "    \n",
    "    # linear decay\n",
    "    new_exploration_rate = MAX_EXPLORATION_RATE - (EXPLORATION_RATE_DECAY) * episode\n",
    "    return max(new_exploration_rate, MIN_EXPLORATION_RATE)\n",
    "\n",
    "def should_explore(exploration_rate):\n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    return exploration_rate > exploration_rate_threshold # exploration rate decreases, less frequent larger than random number\n",
    "\n",
    "def encode(state):\n",
    "    return np.array([np.array([state]).reshape((1, 1))])\n",
    "\n",
    "def get_next_action(q_model, state, exploration_rate):\n",
    "    if should_explore(exploration_rate):\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(q_model.predict(encode(state)))\n",
    "\n",
    "def replay_memory_training(q_model, replay_memory, batch_size = 64):\n",
    "    sample_memory = random.sample(replay_memory, min(len(replay_memory), batch_size))\n",
    "    states = np.array([state for (state, _, _, _, _) in sample_memory])\n",
    "    encoded_states = np.array([encode(state)[0] for state in states])\n",
    "    actions = np.array([action for (_, action, _, _, _) in sample_memory])\n",
    "    new_states = np.array([new_state for (_, _, new_state, _, _) in sample_memory])\n",
    "    encoded_new_states = np.array([encode(new_state)[0] for new_state in new_states])\n",
    "    rewards = np.array([reward for (_, _, _, reward, _) in sample_memory])\n",
    "    dones = np.array([done for (_, _, _, _, done) in sample_memory])\n",
    "    next_values = np.max(q_model.predict(encoded_new_states), axis=1)\n",
    "    targets = np.where(dones, rewards, rewards + y * next_values)\n",
    "    current_targets = q_model.predict(encoded_states)\n",
    "    \n",
    "    for index, action in enumerate(actions):\n",
    "        current_targets[index][action] = targets[index]\n",
    "    \n",
    "    q_model.fit(encoded_states, current_targets, epochs=1, verbose=0)\n",
    "\n",
    "# discrete state & action space\n",
    "q_model = build_model(STATE_SPACE, ACTION_SPACE)\n",
    "\n",
    "# replay memory with size 100\n",
    "replay_memory = deque(maxlen=10000)\n",
    "\n",
    "exploration_rate = MAX_EXPLORATION_RATE\n",
    "total_rewards = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    if episode % 10 == 0:\n",
    "        print('Episode', episode)\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_EPISODE_LENGTH): \n",
    "        action = get_next_action(q_model, state, exploration_rate)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        replay_memory.append((state, action, new_state, reward, done))\n",
    "        replay_memory_training(q_model, replay_memory)\n",
    "        \n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = decay_exploration_rate(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "rewards_per_episodes = np.split(np.array(total_rewards), NUM_EPISODES/10)\n",
    "\n",
    "print('Average reward per 10 episodes:\\n')\n",
    "for index, r in enumerate(rewards_per_episodes):\n",
    "    print((index + 1) * 10, \": \", str(np.average(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8120596, -3.1325543, -2.6421409, -3.1163185, -3.1478245,\n",
       "        -4.2167587]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.predict([454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory with fixed targets\n",
    "Instead of using a variable network for training, update the target network after several episodes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 5,192\n",
      "Trainable params: 5,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0\n",
      "Episode 10\n",
      "Episode 20\n",
      "Episode 30\n",
      "Episode 40\n",
      "Episode 50\n",
      "Episode 60\n",
      "Episode 70\n",
      "Episode 80\n",
      "Episode 90\n",
      "Episode 100\n",
      "Episode 110\n",
      "Episode 120\n",
      "Episode 130\n",
      "Episode 140\n",
      "Episode 150\n",
      "Episode 160\n",
      "Episode 170\n",
      "Episode 180\n",
      "Episode 190\n",
      "Episode 200\n",
      "Episode 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x13d855dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 536, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1277, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-9a719824c802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mreplay_memory_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mtrain_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-167-9a719824c802>\u001b[0m in \u001b[0;36mreplay_memory_training\u001b[0;34m(q_model, target_model, replay_memory, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_memory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_memory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mq_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_new_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mcurrent_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "def build_model(state_space, action_space):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(state_space, 10, input_length=1))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action_space))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='mae', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "np.random.seed(1234)\n",
    "env.seed(1234)\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "STATE_SPACE = env.observation_space.n\n",
    "ACTION_SPACE = env.action_space.n\n",
    "\n",
    "NUM_EPISODES = 500\n",
    "MAX_EPISODE_LENGTH = 20\n",
    "\n",
    "y = 0.99 # gamma = decay rate\n",
    "TAU = 1e-2 # learning rate for target model parameter updates - similar to alpha in q_table\n",
    "\n",
    "MAX_EXPLORATION_RATE = 1\n",
    "MIN_EXPLORATION_RATE = 0.01\n",
    "EXPLORATION_RATE_DECAY = 0.01\n",
    "\n",
    "def decay_exploration_rate(episode):\n",
    "    # exponential decay\n",
    "    #return MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLORATION_RATE_DECAY * episode)\n",
    "    \n",
    "    # linear decay\n",
    "    new_exploration_rate = MAX_EXPLORATION_RATE - (EXPLORATION_RATE_DECAY) * episode\n",
    "    return max(new_exploration_rate, MIN_EXPLORATION_RATE)\n",
    "\n",
    "    # reduce by EXPLORATION_RATE_DECAY\n",
    "    #new_exploration_rate = MAX_EXPLORATION_RATE * pow((1 - EXPLORATION_RATE_DECAY), episode)\n",
    "    #return max(new_exploration_rate, MIN_EXPLORATION_RATE)\n",
    "\n",
    "def should_explore(exploration_rate):\n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    return exploration_rate > exploration_rate_threshold # exploration rate decreases, less frequent larger than random number\n",
    "\n",
    "def encode(state):\n",
    "    return np.array([np.array([state]).reshape((1, 1))])\n",
    "\n",
    "def get_next_action(q_model, state, exploration_rate):\n",
    "    if should_explore(exploration_rate):\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(q_model.predict(encode(state)))\n",
    "\n",
    "def replay_memory_training(q_model, target_model, replay_memory, batch_size = 64):\n",
    "    sample_memory = random.sample(replay_memory, min(len(replay_memory), batch_size))\n",
    "    states = np.array([state for (state, _, _, _, _) in sample_memory])\n",
    "    encoded_states = np.array([encode(state)[0] for state in states])\n",
    "    actions = np.array([action for (_, action, _, _, _) in sample_memory])\n",
    "    new_states = np.array([new_state for (_, _, new_state, _, _) in sample_memory])\n",
    "    encoded_new_states = np.array([encode(new_state)[0] for new_state in new_states])\n",
    "    rewards = np.array([reward for (_, _, _, reward, _) in sample_memory])\n",
    "    dones = np.array([done for (_, _, _, _, done) in sample_memory])\n",
    "    q_future = np.max(target_model.predict(encoded_new_states), axis=1)\n",
    "    targets = np.where(dones, rewards, rewards + y * q_future)\n",
    "    current_targets = target_model.predict(encoded_states)\n",
    "    \n",
    "    for index, action in enumerate(actions):\n",
    "        current_targets[index][action] = targets[index]\n",
    "    \n",
    "    q_model.fit(encoded_states, current_targets, epochs=1, verbose=0)\n",
    "    \n",
    "def train_target_model(q_model, target_model, t=TAU):\n",
    "    q_model_weights = q_model.get_weights()\n",
    "    target_model_weights = target_model.get_weights()\n",
    "    for i in range(len(target_model_weights)):\n",
    "        target_model_weights[i] = (1 - t) * target_model_weights[i] + t * q_model_weights[i]\n",
    "    target_model.set_weights(target_model_weights)\n",
    "\n",
    "# discrete state & action space\n",
    "q_model = build_model(STATE_SPACE, ACTION_SPACE)\n",
    "target_model = tf.keras.models.clone_model(q_model)\n",
    "\n",
    "# replay memory with size 100\n",
    "replay_memory = deque(maxlen=1000)\n",
    "\n",
    "exploration_rate = MAX_EXPLORATION_RATE\n",
    "total_rewards = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    if episode % 10 == 0:\n",
    "        print('Episode', episode)\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_EPISODE_LENGTH): \n",
    "        action = get_next_action(q_model, state, exploration_rate)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        replay_memory.append((state, action, new_state, reward, done))\n",
    "        replay_memory_training(q_model, target_model, replay_memory)\n",
    "        train_target_model(q_model, target_model, TAU)\n",
    "        \n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = decay_exploration_rate(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "rewards_per_episodes = np.split(np.array(total_rewards), NUM_EPISODES/10)\n",
    "\n",
    "print('Average reward per 10 episodes:\\n')\n",
    "for index, r in enumerate(rewards_per_episodes):\n",
    "    print((index + 1) * len(r), \": \", str(np.avg(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00341365 -0.0028316  -0.00295588 -0.00309611 -0.00400039 -0.00292171]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(q_model.predict([454]))\n",
    "print(np.argmax(q_model.predict([454])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 3, 2]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in env.decode(454)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
